{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH02_Loss-functions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scharnk/Linear-Classifiers-in-Python/blob/master/CH02_Loss_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paVBJvh5qFjY",
        "colab_type": "text"
      },
      "source": [
        "Dot product is multiplication in higher dimensions\n",
        "each value in an array is multiplied by the value in the same position of the other array\n",
        " \n",
        " # Linear Classifier Prediction\n",
        " \n",
        "* raw model output = coefficients * features + intercept\n",
        "* linear classifier prediction: always check the sign \n",
        ">* if positive -- predict one class\n",
        ">* if negative -- predict the other class\n",
        "* Same for logistic regression and linear SVM\n",
        ">* **fit is different but predict is the same**\n",
        "\n",
        "along a decision boundary the model will = 0 <br>\n",
        "above it = positive <br>\n",
        "below it = negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzyko1aopsGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the coefficients\n",
        "model.coef_ = np.array([[-5,3]])\n",
        "model.intercept_ = np.array([-11])\n",
        "\n",
        "# Plot the data and decision boundary\n",
        "plot_classifier(X,y,model)\n",
        "\n",
        "# Print the number of errors\n",
        "num_err = np.sum(y != model.predict(X))\n",
        "print(\"Number of errors:\", num_err)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcq_gmWf2O2Y",
        "colab_type": "text"
      },
      "source": [
        "# Least squares: the squared loss\n",
        "\n",
        "* scikit-learn's LinearRegression minimizes a loss\n",
        "* error is defined as the difference between the true target value and the predicted target value\n",
        "* Minimization is with respect to coefficients or parameters of the model.\n",
        "* Note that in scikit-learn model.score() isn't necessarily the loss function.\n",
        "\n",
        "\n",
        "# Classification errors: the 0-1 loss\n",
        "\n",
        "* Squared loss not appropriate for classification problems\n",
        "* A natural loss for classification problem is the number of errors:\n",
        ">* This is the 0-1 loss: it's 0 for a correct prediction and 1 for an incorrect prediction.\n",
        ">* but this loss is hard to minimize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hImh4Nap1tZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The squared error, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(y.size):\n",
        "        # Get the true and predicted target values for example 'i'\n",
        "        y_i_true = y[i]\n",
        "        y_i_pred = w@X[i]\n",
        "        s = s + (y_i_true - y_i_pred)**2\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LinearRegression coefficients\n",
        "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
        "print(lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I98KcdUi9VQ8",
        "colab_type": "text"
      },
      "source": [
        "# Loss function\n",
        "\n",
        "* linear regression bad since classification is binary\n",
        "* logisic loss smooth version of 0-1 loss (slowly decreases)\n",
        "* hinge loss used in SVM linear to flat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M9Y-cqu1tMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mathematical functions for logistic and hinge losses\n",
        "def log_loss(raw_model_output):\n",
        "   return np.log(1+np.exp(-raw_model_output))\n",
        "def hinge_loss(raw_model_output):\n",
        "   return np.maximum(0,1-raw_model_output)\n",
        "\n",
        "# Create a grid of values and plot\n",
        "grid = np.linspace(-2,2,1000)\n",
        "plt.plot(grid, log_loss(grid), label='logistic')\n",
        "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEedYP9gBgBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The logistic loss, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(len(X)):\n",
        "        raw_model_output = w@X[i]\n",
        "        s = s + log_loss(raw_model_output * y[i])\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LogisticRegression\n",
        "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}