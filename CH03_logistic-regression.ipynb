{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH03_ Logistic-regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scharnk/Linear-Classifiers-in-Python/blob/master/CH03_logistic-regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU5OBojJLV_u",
        "colab_type": "text"
      },
      "source": [
        "# Regularized Logistic Regression\n",
        "\n",
        "in scikit learn the hyperparameter 'C' is the inverse of the regularization strength\n",
        "* larger C means less regularization\n",
        "* smaller C means more\n",
        "<br><br>\n",
        "* regularized loss = original loss + large coefficient penalty\n",
        "* **more regularization: lower training accuracy**\n",
        "* **more regularization: (almost always) higher test accuracy**\n",
        "<br><br>\n",
        "* if large features were causing overfitting, regularization causes less overfitting from diminishing feature size\n",
        "\n",
        "# L1 vs L2 regularization\n",
        "\n",
        "Lasso = linear regression with L1 regularization <br>\n",
        "Ridge = linear regression with L2 regularization <br>\n",
        "For other models like logistic regression we just say L1, L2, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEUx5C_SvAtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and validaton errors initialized as empty list\n",
        "train_errs = list()\n",
        "valid_errs = list()\n",
        "\n",
        "# Loop over values of C_value\n",
        "for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
        "    # Create LogisticRegression object and fit\n",
        "    lr = LogisticRegression(C=C_value)\n",
        "    lr.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate error rates and append to lists\n",
        "    train_errs.append( 1.0 - lr.score(X_train, y_train) )\n",
        "    valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )\n",
        "    \n",
        "# Plot results\n",
        "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
        "plt.legend((\"train\", \"validation\"))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4KP6IXOrbWW",
        "colab_type": "text"
      },
      "source": [
        "# Logistic regression and feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA_lzjxxrJhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify L1 regularization\n",
        "lr = LogisticRegression(penalty='l1')\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
        "searcher.fit(X_train, y_train)\n",
        "\n",
        "# Report the best parameters\n",
        "print(\"Best CV params\", searcher.best_params_)\n",
        "\n",
        "# Find the number of nonzero coefficients (selected features)\n",
        "best_lr = searcher.best_estimator_\n",
        "coefs = best_lr.coef_\n",
        "print(\"Total number of features:\", coefs.size)\n",
        "print(\"Number of selected features:\", np.count_nonzero(coefs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkvvRksCrJcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the indices of the sorted cofficients\n",
        "inds_ascending = np.argsort(lr.coef_.flatten()) \n",
        "inds_descending = inds_ascending[::-1]\n",
        "\n",
        "# Print the most positive words\n",
        "print(\"Most positive words: \", end=\"\")\n",
        "for i in range(5):\n",
        "    print(vocab[inds_descending[i]], end=\", \")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print most negative words\n",
        "print(\"Most negative words: \", end=\"\")\n",
        "for i in range(5):\n",
        "    print(vocab[inds_ascending[i]], end=\", \")\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIow1mA5AyZz",
        "colab_type": "text"
      },
      "source": [
        "How are these probabilities computed?\n",
        "* logistic regression predictions: sign of raw model output\n",
        "* logistic regression probabilities: \"squashed\" raw model output\n",
        "* **USE SIGMOID FUNCTION** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2wpYusRrJPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the regularization strength\n",
        "model = LogisticRegression(C=____)\n",
        "\n",
        "# Fit and plot\n",
        "model.fit(X,y)\n",
        "plot_classifier(X,y,model,proba=True)\n",
        "\n",
        "# Predict probabilities on training points\n",
        "prob = model.predict_proba(X)\n",
        "print(\"Maximum predicted probability\", np.max(prob))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG3Zq-7ACn9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the regularization strength\n",
        "model = LogisticRegression(C=0.1)\n",
        "\n",
        "# Fit and plot\n",
        "model.fit(X,y)\n",
        "plot_classifier(X,y,model,proba=True)\n",
        "\n",
        "# Predict probabilities on training points\n",
        "prob = model.predict_proba(X)\n",
        "print(\"Maximum predicted probability\", np.max(prob))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVx8fYdlDHBE",
        "colab_type": "text"
      },
      "source": [
        "As you probably noticed, smaller values of C lead to less confident predictions. That's because smaller C means more regularization, which in turn means smaller coefficients, which means raw model outputs closer to zero and, thus, probabilities closer to 0.5 after the raw model output is squashed through the sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_KluRUkCo76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X,y)\n",
        "\n",
        "# Get predicted probabilities\n",
        "proba = lr.predict_proba(X)\n",
        "\n",
        "# Sort the example indices by their maximum probability\n",
        "proba_inds = np.argsort(np.max(proba,axis=1))\n",
        "\n",
        "# Show the most confident (least ambiguous) digit\n",
        "show_digit(proba_inds[-1], lr)\n",
        "\n",
        "# Show the least confident (most ambiguous) digit\n",
        "show_digit(proba_inds[0], lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgtazOXEFgaz",
        "colab_type": "text"
      },
      "source": [
        "Combining binary classifiers with one-vs-rest\n",
        "* one-vs-rest is **default** for scikit learn's logistic regression\n",
        "\n",
        "# One-vs-rest:\n",
        "\n",
        "* fit a binary classifier for each class\n",
        "* predict with all, take largest output\n",
        "* pro: simple, modular\n",
        "* con: not directly optimizing accuracy\n",
        "* common for SVMs as well\n",
        "* can produce probabilities\n",
        "\n",
        "# \"Multinomial\" or \"softmax\":\n",
        "\n",
        "* fit a single classifier for all classes\n",
        "* prediction directly outputs best class\n",
        "* con: more complicated, new code\n",
        "* pro: tackle the problem directly\n",
        "* possible for SVMs, but less common\n",
        "* can produce probabilities\n",
        "\n",
        "# Model coefficients for multi-class\n",
        "\n",
        "require non-default solver such as \"lbfgs\"<br>\n",
        "example below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHrKWxsrCowd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "In [1]: lr_ovr = LogisticRegression() # one-vs-rest by default\n",
        "\n",
        "In [2]: lr_ovr.fit(X,y)\n",
        "\n",
        "In [3]: lr_ovr.coef_.shape\n",
        "Out[3]: (3,13)\n",
        "\n",
        "In [4]: lr_ovr.intercept_.shape\n",
        "Out[4]: (3,)\n",
        " \n",
        "In [5]: lr_mn = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\")\n",
        "\n",
        "In [6]: lr_mn.fit(X,y)\n",
        "\n",
        "In [7]: lr_mn.coef_.shape\n",
        "Out[7]: (3,13)\n",
        "\n",
        "In [8]: lr_mn.intercept_.shape\n",
        "Out[8]: (3,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csQglQseM2Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit one-vs-rest logistic regression classifier\n",
        "lr_ovr = LogisticRegression()\n",
        "lr_ovr.fit(X_train, y_train)\n",
        "\n",
        "print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
        "print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
        "\n",
        "# Fit softmax classifier\n",
        "lr_mn = lr_mn = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\")\n",
        "lr_mn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
        "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsVOGA9aM3Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print training accuracies\n",
        "print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
        "print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
        "\n",
        "# Create the binary classifier (class 1 vs. rest)\n",
        "lr_class_1 = LogisticRegression(C=100)\n",
        "lr_class_1.fit(X_train, y_train==1)\n",
        "\n",
        "# Plot the binary classifier (class 1 vs. rest)\n",
        "plot_classifier(X_train, y_train==1, lr_class_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2vZ6suSO_CF",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the binary classifier incorrectly labels almost all points in class 1 (shown as red triangles in the final plot)! Thus, this classifier is not a very effective component of the one-vs-rest classifier. In general, though, one-vs-rest often works well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcTIFJOeM3Rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll use SVC instead of LinearSVC from now on\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create/plot the binary classifier (class 1 vs. rest)\n",
        "svm_class_1 = SVC()\n",
        "svm_class_1.fit(X_train, y_train==1)\n",
        "plot_classifier(X_train, y_train==1, svm_class_1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}