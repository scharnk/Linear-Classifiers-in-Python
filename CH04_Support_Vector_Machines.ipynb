{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH04_Support-Vector-Machines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scharnk/Linear-Classifiers-in-Python/blob/master/CH04_Support_Vector_Machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7SAYPB2wudQ",
        "colab_type": "text"
      },
      "source": [
        "What is an SVM?\n",
        "* Linear classifiers (so far)\n",
        "* Trained using the hinge loss and L2 regularization\n",
        "\n",
        "What are support vectors?\n",
        "* Support vector: a training example not in the flat part of the loss diagram\n",
        "* Support vector: an example that is incorrectly classified or close to the boundary\n",
        "* If an example is not a support vector, removing it has no effect on the model\n",
        "* Having a small number of support vectors makes kernel SVMs really fast\n",
        "\n",
        "# Max-margin viewpoint\n",
        "* The SVM maximizes the \"margin\" for linearly separable datasets\n",
        "* Margin: distance from the boundary to the closest points\n",
        "\n",
        "* SVM = hinge loss with L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLl2a1nlwcId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train a linear SVM\n",
        "svm = SVC(kernel=\"linear\")\n",
        "svm.fit(X,y)\n",
        "plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
        "\n",
        "# Make a new data set keeping only the support vectors\n",
        "print(\"Number of original examples\", len(X))\n",
        "print(\"Number of support vectors\", len(svm.support_))\n",
        "X_small = X[svm.support_]\n",
        "y_small = y[svm.support_]\n",
        "\n",
        "# Train a new SVM using only the support vectors\n",
        "svm_small = SVC(kernel=\"linear\")\n",
        "svm_small.fit(X_small, y_small)\n",
        "plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3lAVzUZ1K3A",
        "colab_type": "text"
      },
      "source": [
        "# Transforming features\n",
        "* can transform features to make points linearly separeable\n",
        "i.e. transformed feature=(original feature)^2\n",
        "\n",
        "* **fitting a linear model in a transformed space corresponds to a non-linear model in the original space**\n",
        "\n",
        "# kernals and SVM kernals implement feature transformations in a computationally efficiently way\n",
        "* **default is radial basis func or \"RBF\"**\n",
        "# Gamma (hyperparameter)\n",
        "* gamma default = 1 \n",
        "* gamma controls fitting\n",
        "* too large = overfitting\n",
        "* too small = underfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81vrUrUPVYvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an RBF SVM\n",
        "svm = SVC()\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
        "searcher = GridSearchCV(svm, parameters)\n",
        "searcher.fit(X,y)\n",
        "\n",
        "# Report the best parameters\n",
        "print(\"Best CV params\", searcher.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_x78qJHVYeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an RBF SVM\n",
        "svm = SVC()\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
        "searcher = GridSearchCV(svm, parameters)\n",
        "searcher.fit(X_train, y_train)\n",
        "\n",
        "# Report the best parameters and the corresponding score\n",
        "print(\"Best CV params\", searcher.best_params_)\n",
        "print(\"Best CV accuracy\", searcher.best_score_)\n",
        "\n",
        "# Report the test accuracy using these best parameters\n",
        "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW0ilYg1bxHo",
        "colab_type": "text"
      },
      "source": [
        "## Pros and Cons (Logistic-Regression vs SVM)\n",
        "\n",
        "### **Logistic regression:**\n",
        "\n",
        "* Is a linear classifier\n",
        "* Can use with kernels, but slow\n",
        "* Outputs meaningful probabilities\n",
        "* Can be extended to multi-class\n",
        "* All data points affect fit\n",
        "* L2 or L1 regularization\n",
        "\n",
        "### **Support vector machine (SVM):**\n",
        "\n",
        "* Is a linear classifier\n",
        "* Can use with kernels, and fast\n",
        "* Does not naturally output probabilities\n",
        "* Can be extended to multi-class\n",
        "* Only \"support vectors\" affect fit\n",
        "* Conventionally just L2 regularization\n",
        "\n",
        "<br>\n",
        "\n",
        "## Logistic regression in sklearn:\n",
        "\n",
        "* linear_model.LogisticRegression\n",
        "\n",
        "### Key hyperparameters in sklearn:\n",
        "\n",
        "* C (inverse regularization strength)\n",
        "* penalty (type of regularization)\n",
        "* multi_class (type of multi-class)\n",
        "<br>\n",
        "\n",
        "## SVM in sklearn:\n",
        "\n",
        "* svm.LinearSVC and svm.SVC\n",
        "\n",
        "### Key hyperparameters in sklearn:\n",
        "\n",
        "* C (inverse regularization strength)\n",
        "* kernel (type of kernel)\n",
        "* gamma (inverse RBF smoothness)\n",
        "\n",
        "# SGDClassifier\n",
        "### SGDClassifier: scales well to large datasets\n",
        "In [1]: from sklearn.linear_model import SGDClassifier <br>\n",
        "In [2]: logreg = SGDClassifier(loss='log') <br>\n",
        "In [3]: linsvm = SGDClassifier(loss='hinge') <br>\n",
        "* SGDClassifier hyperparameter alpha is like 1/C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikf1DwHkVYUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We set random_state=0 for reproducibility \n",
        "linear_classifier = SGDClassifier(random_state=0)\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
        "             'loss':['hinge','log'], 'penalty':['l1','l2']}\n",
        "searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
        "searcher.fit(X_train, y_train)\n",
        "\n",
        "# Report the best parameters and the corresponding score\n",
        "print(\"Best CV params\", searcher.best_params_)\n",
        "print(\"Best CV accuracy\", searcher.best_score_)\n",
        "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuEDNAHqQtnF",
        "colab_type": "text"
      },
      "source": [
        "### How does this course fit into Data Science?\n",
        "* Data science\n",
        "* --> Machine learning\n",
        "* --> --> Supervised learning\n",
        "* --> --> --> Classification\n",
        "* --> --> --> --> Linear classifiers (this course)"
      ]
    }
  ]
}